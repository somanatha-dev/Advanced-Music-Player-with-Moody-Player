import { useEffect, useRef, useState } from "react";
import * as faceapi from "face-api.js";

function App() {
  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const [loading, setLoading] = useState(true);
  const [expression, setExpression] = useState("");

  // For flicker reduction
  const lastExpressionRef = useRef("");
  const sameExpressionCountRef = useRef(0);

  const loadModels = async () => {
    setLoading(true);
    const MODEL_URL = "/models";
    await Promise.all([
      faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
      faceapi.nets.faceExpressionNet.loadFromUri(MODEL_URL),
    ]);
    setLoading(false);
  };

  const startVideo = () => {
    navigator.mediaDevices
      .getUserMedia({ video: {} })
      .then((stream) => {
        videoRef.current.srcObject = stream;
      })
      .catch((err) => console.error("Camera error:", err));
  };

  const handleVideoPlay = () => {
    const interval = setInterval(async () => {
      if (!videoRef.current || videoRef.current.readyState !== 4) return;

      const video = videoRef.current;
      const canvas = canvasRef.current;

      const displaySize = {
        width: video.offsetWidth,
        height: video.offsetHeight,
      };

      const detections = await faceapi
        .detectAllFaces(
          video,
          new faceapi.TinyFaceDetectorOptions({ inputSize: 224, scoreThreshold: 0.5 })
        )
        .withFaceExpressions();

      faceapi.matchDimensions(canvas, displaySize);
      const resizedDetections = faceapi.resizeResults(detections, displaySize);

      const ctx = canvas.getContext("2d");
      ctx.clearRect(0, 0, canvas.width, canvas.height);

      faceapi.draw.drawDetections(canvas, resizedDetections);

      // Expression smoothing
      if (detections.length > 0) {
        const expressions = detections[0].expressions;
        const maxExp = Object.keys(expressions).reduce((a, b) =>
          expressions[a] > expressions[b] ? a : b
        );

        if (maxExp === lastExpressionRef.current) {
          sameExpressionCountRef.current += 1;
        } else {
          sameExpressionCountRef.current = 0;
          lastExpressionRef.current = maxExp;
        }

        if (sameExpressionCountRef.current >= 3) {
          setExpression(`${maxExp} (${expressions[maxExp].toFixed(2)})`);
        }
      }
    }, 200);

    return () => clearInterval(interval);
  };

  useEffect(() => {
    document.body.style.margin = "0";
    document.body.style.height = "100vh";
    document.body.style.backgroundColor = "#111";
    loadModels();
    startVideo();
  }, []);

  return (
    <div
      style={{
        display: "flex",
        flexDirection: "column",
        alignItems: "center",
        justifyContent: "center",
        height: "100vh",
        width: "100vw",
        backgroundColor: "#111",
        color: "#fff",
        position: "relative",
      }}
    >
      <h1 style={{ textAlign: "center" }}>Face Detection</h1>
      {loading && <p>Loading models...</p>}
      <div style={{ position: "relative" }}>
        <video
          ref={videoRef}
          autoPlay
          muted
          playsInline
          onPlay={handleVideoPlay}
          style={{
            border: "2px solid black",
            width: "500px",
            height: "auto",
          }}
        />
        <canvas
          ref={canvasRef}
          style={{
            position: "absolute",
            top: 0,
            left: 0,
          }}
        />
        {!loading && expression && (
          <p
            style={{
              position: "absolute",
              bottom: "-30px",
              left: "50%",
              transform: "translateX(-50%)",
              color: "#0f0",
              fontWeight: "bold",
              fontSize: "18px",
            }}
          >
            {expression}
          </p>
        )}
      </div>
    </div>
  );
}

export default App;